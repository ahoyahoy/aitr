Zapojíme **mastra.ai tools** do dvou míst: **(A) ingest/ETL** a **(B) query-time retrieval**. Cíl: mastra = canonical embedder + chopper; Upstash Vector = úložiště; Redis = cache a verze.

Mastra.ai řeže a embeduje veškeré travel texty (POI/guide/review/pattern), ukládá je do Upstash Vector, kombinuje se slovníkovým BM25 a rerankerem, a nad tím osobně plánuje itineráře přes mem-profil uživatele; vše s verzováním, cache a low-latency rozhraním.

_Cíl: Rychlá a přesná travel doporučení a itineráře._

Architektura:

- A) Ingestion/ETL – Normalize → Chunk (recursive 512–768, 15–20 % overlap) → Embed (mastra) → Index (Upstash Vector). Idempotence přes emb_hash, cache v Redis, verzování modelu a backfill přes QStash.

- B) Query-time – Query rewrite/expand (mastra), hybrid retrieval (Vector + BM25), cross-encoder rerank top-50, volitelný on-the-fly chunking pro dlouhé vstupy.

- Personalizace (mem0) – Profilové preference embedu-jeme, při reranku zohledníme podobnost k uživateli.

- Observabilita – log modelVer, latence chop/embed/search, cost guardrails.

Úložiště: Upstash Vector = jediný zdroj vektorů; Redis = cache, verze, token-bucket.

# Pipeline

## A) Ingestion / ETL (offline, po normalizaci)

1. **Normalize → Chunk → Embed → Index**

   * Po sjednocení entity (`Place`, `Guide`, `Review`, `Pattern`) spustíme **mastra.chop()** nad textovým polem (popisy, recenze, průvodce).
   * Chunking: *recursive sentence → token cap* (např. 512–768 tok., overlap 15–20 %), fallback *word boundary*.
   * Přidáme metadata: `{placeId, h3, lang, source, section, version, model, hash}`.
   * Každý chunk → **mastra.embed()** → uložíme do **Upstash Vector** (collection `places|guides|reviews|patterns`).
2. **Idempotence & cache**

   * `emb_hash = sha256(model+lang+clean(text))`.
   * Pokud `emb_cache:{emb_hash}` v Redis existuje → přeskočíme embed; jinak po indexaci nastavíme TTL.
3. **Verzování**

   * `embed_model_ver` v metadatech (např. `mstr-e5-large-v3@2025-05`).
   * Při upgradu modelu umíme backfill přes QStash (batch re-embed job).

## B) Query-time retrieval (online)

1. **Query rewrite/expand**

   * `mastra.rewrite(query)` pro synonymizaci a multi-lingual varianty (pokud využijeme tools); uložíme `q_variants`.
2. **Hybrid retrieval**

   * `Upstash Vector.search(q_emb)` (primární) + **BM25** (Typesense/Meili) pro přesné názvy.
   * **Rerank** (cross-encoder) na top-50.
3. **On-the-fly chunking (volitelné)**

   * Když uživatel vloží dlouhý prompt (např. vlastní itinerář), uděláme **mastra.chop()** a **mastra.embed()** ad-hoc do dočasné kolekce `session:{id}` s TTL.
4. **mem0**

   * Profilové „preference sentences“ i feedback notes embedujeme přes **mastra.embed()** do kolekce `user_profiles`; při personalizovaném reranku děláme dotaz `similar(user_profile_vec, item_vec)`.

# Parametry a zásady

* **Chunk size**: 512–768 tok., overlap 15–20 %.
* **Fields**: oddělené sekce (`description`, `tips`, `opening_rules`, `review.text`) → samostatné chupy s `section` metadaty.
* **Jazyky**: `lang` zjišťujeme (fastlang); u vícejazyčných textů embedujeme každou jazykovou část zvlášť.
* **Dedup**: před embedem `minhash`/`simhash` na text → filtrace near-duplicates.
* **QoS**: rate-limit embeds (token bucket v Redis), retry s jitterem; při výpadku **fallback** na Upstash built-in embedder.

# Schéma metadat (Upstash Vector)

```json
{
  "id": "guide:melb:fitzroy:42#ch3",
  "text": "Best coffee walk through Fitzroy…",
  "vector": [ ... ],
  "metadata": {
    "type": "guide|place|review|pattern|user_profile",
    "placeId": "gmaps:ChIJ…",
    "h3": "892a…",
    "lang": "en",
    "section": "description|tips|review",
    "source": "gmaps|booking|custom",
    "model": "mastra-e5-large-v3",
    "modelVer": "2025-05",
    "hash": "sha256:…",
    "createdAt": 1699999999
  }
}
```

# Napojení do našich bloků

* **Data ingestion**: po `Place/Guide/Review` normalizaci → `mastra.chop+embed` → Upstash Vector.
* **Knowledge/embeddings**: mastra je primární embedder; kombinujeme s BM25 a rerankerem.
* **mem0**: profilové věty a feedback poznámky embedujeme přes mastra; používáme pro personalized rerank.
* **PT/Planning/OR-Tools**: textové části (popisy POI, patterny) přichází z vektorového vyhledávání; časy/trasování jde mimo (OSRM/H3/Google).
* **Observabilita**: logujeme `modelVer`, latence `chop`, `embed`, počet chunků; udržujeme cost guardrail (max tokenů/ingest).

# Migrace / backfill

* Batch job přes QStash: načteme nefembedované dokumenty, běžíme `chop+embed` s limitem RPS.
* Při změně modelu zvýšíme `modelVer`, spustíme postupný re-embed pouze u top-n navštěvovaných dokumentů (80/20), zbytek lazy při prvním dotazu.

# Shrnutí

* **Mastra.ai** řeší **sekaní textů a embedding** všude, kde pracujeme s dlouhými texty (guides, reviews, patterns, user mem0).
* **Upstash Vector** je jediné úložiště vektorů; **Redis** drží cache, hashe a verze.
* **Hybrid retrieval** (semantic + BM25) a **personalizace** (mem0 embeddings) běží na těchto vektorech.


# Minimal integrace pseudokod

```ts
// ETL krok po normalizaci
for (const guide of normalizedGuides) {
  await chopAndEmbed({
    id: `guide:${guide.city}:${guide.slug}`,
    text: guide.description + '\n' + guide.tips?.join('\n') ?? '',
    meta: { type: 'guide', h3: guide.h3, lang: guide.lang, source: guide.source }
  })
}
```

```ts
// Query-time: hybrid retrieval
export async function retrieveCandidates(q: string, filters: any) {
  const qEmb = await m.embed([q]).then(r => r[0])
  const sem = await upstashVector.search({ vector: qEmb, topK: 100, where: filters })
  const bm = await typesense.search(q, { filters })
  const merged = mergeAndRerank(sem, bm) // cross-encoder rerank
  return merged.slice(0, 50)
}
```

# Mastra Embedding

## 1) Vector store init (UpstashVector)

```ts
// server/utils/vector.ts
import { UpstashVector } from '@mastra/upstash'

export const vector = new UpstashVector({
  url: process.env.UPSTASH_VECTOR_URL!,
  token: process.env.UPSTASH_VECTOR_TOKEN!
})

// indexy (namespace) které používáme
export const INDEX = {
  PLACES: 'vectors:places',
  GUIDES: 'vectors:guides',
  REVIEWS: 'vectors:reviews',
  PATTERNS: 'vectors:patterns',
  USER_PROFILES: 'vectors:user_profiles'
}
```
---

## 2) Chunk & embed utilita (Mastra RAG + AI SDK)

```ts
// server/utils/mastra.ts
import { MDocument } from '@mastra/rag'
import { embedMany } from 'ai'
import { openai } from '@ai-sdk/openai'
import crypto from 'node:crypto'
import { redis } from './redis' 
import { vector, INDEX } from './vector'

type ChunkEmbedInput = {
  id: string
  text: string
  meta: Record<string, any>
  indexName: string
  // defaulty držíme rozumné podle mastra příkladů
  chunk?: {
    strategy?: 'recursive'|'sentence'|'markdown'|'html'|'json'
    maxSize?: number
    overlap?: number
    // markdown/headers apod. si můžeme doplnit podle typu
    headers?: [string, string][]
  }
  embed?: {
    model?: string // např. 'text-embedding-3-small'
    dimensions?: number // např. 256 (OpenAI text-embedding-3 supports)
  }
}

export async function chunkAndEmbed(input: ChunkEmbedInput) {
  // idempotence přes hash obsahu + verzi modelu
  const model = input.embed?.model ?? 'text-embedding-3-small'
  const dims = input.embed?.dimensions
  const contentHash = crypto
    .createHash('sha256')
    .update(`${model}:${dims ?? 'full'}:${input.text}`)
    .digest('hex')

  const cacheKey = `emb:${input.indexName}:${input.id}:${contentHash}`
  if (await redis.exists(cacheKey)) return { status: 'cached' }

  // 1)
  const doc = MDocument.fromText(input.text)

  // 2) chunking
  const chunks = await doc.chunk({
    strategy: input.chunk?.strategy ?? 'recursive',
    maxSize: input.chunk?.maxSize ?? 512,
    overlap: input.chunk?.overlap ?? 50,
    headers: input.chunk?.headers
  })

  // 3) embedMany přes AI SDK
  const { embeddings } = await embedMany({
    model: openai.embedding(model, dims ? { dimensions: dims } : undefined),
    values: chunks.map(c => c.text)
  })

  // 4) upsert do Upstash Vector (dense; hybrid sparse lze doplnit později)
  await vector.upsert({
    indexName: input.indexName,
    vectors: embeddings as number[][],
    metadata: chunks.map((c, i) => ({
      ...input.meta,
      chunkIndex: i,
      text: c.text // pro debug
    })),
    ids: chunks.map((_, i) => `${input.id}#ch${i}`)
  })

  await redis.setex(cacheKey, 60 * 60 * 24 * 30, '1')
  return { status: 'ok', chunks: chunks.length }
}
```

* `MDocument.fromText(...).chunk({...})` a volby strategie/velikosti vychází z Mastra referencí a příkladů.
* Embeddingy přes `embedMany` (AI SDK) a OpenAI provider odpovídají jejich doporučenému vzoru; možnost nastavit `dimensions`.

---

## 3) Retrieval (dotaz → embed → vektorový dotaz + BM25 fallback)

```ts
// server/utils/retrieval.ts
import { embed } from 'ai'
import { openai } from '@ai-sdk/openai'
import { vector } from './vector'
// volitelné: BM25 fallback
import Typesense from 'typesense'

const typesense = new Typesense.Client({
  nodes: [{ host: process.env.TYPESENSE_HOST!, port: 443, protocol: 'https' }],
  apiKey: process.env.TYPESENSE_API_KEY!
})

type RetrieveOpts = {
  indexName: string
  q: string
  topK?: number
  filter?: Record<string, any> // UpstashVector metadata filter
}

export async function retrieveCandidates(opts: RetrieveOpts) {
  const { embedding } = await embed({
    model: openai.embedding('text-embedding-3-small'),
    value: opts.q
  })

  // primární semantické hledání v UpstashVector
  const sem = await vector.query({
    indexName: opts.indexName,
    queryVector: embedding as number[],
    topK: opts.topK ?? 100,
    filter: opts.filter
  })

  // BM25 fallback přes Typesense pro přesné názvy/klíčová slova
  // (kolekci/filtry si držíme paralelně s vektory)
  const bm = await typesense
    .collections(opts.indexName.replace('vectors:', 'bm25:')) // konvence
    .documents()
    .search({ q: opts.q, query_by: 'text,title,tags', per_page: 50 })

  // jednoduchá fúze: RRF (Reciprocal Rank Fusion)
  const rrf = new Map<string, { score: number; meta: any }>()
  const push = (arr: any[], weight = 1) =>
    arr.forEach((item: any, i: number) => {
      const id = item.id ?? item.document?.id
      const meta = item.metadata ?? item.document
      const inc = weight * 1 / (60 + i) // 60 = k
      rrf.set(id, { score: (rrf.get(id)?.score ?? 0) + inc, meta })
    })

  push(sem, 2) // preferujeme semantiku
  push(bm.hits ?? [], 1)

  return [...rrf.entries()]
    .sort((a, b) => b[1].score - a[1].score)
    .slice(0, opts.topK ?? 50)
    .map(([id, v]) => ({ id, score: v.score, metadata: v.meta }))
}
```

* Vzor dotazu do **UpstashVector.query** (dense) je dle reference; Upstash podporuje i hybrid s `sparseVector` a RRF. My tu děláme hybrid fúzi přes BM25+RRF na aplikační vrstvě (jednoduché, levné).
* BM25 fallback (Typesense/Meili) máme v plánu už dřív — tady je to přímo použitelné.

---

## 4) Ingestion endpoint (idempotentní ETL krok)

```ts
// server/routes/ingest.post.ts (Nuxt/Nitro)
import { defineEventHandler, readBody } from 'h3'
import { redis } from '~/server/utils/redis'
import { chunkAndEmbed } from '~/server/utils/mastra'
import { INDEX } from '~/server/utils/vector'

export default defineEventHandler(async (e) => {
  const body = await readBody<{
    id: string
    type: 'guide'|'place'|'review'|'pattern'
    text: string
    meta?: Record<string, any>
  }>(e)

  // idempotence na úrovni ETL (unikátní zdroj + verze)
  const key = `etl:${body.type}:${body.id}`
  if (await redis.get(key)) return { status: 'skipped' }

  const indexName =
    body.type === 'guide' ? INDEX.GUIDES
    : body.type === 'place' ? INDEX.PLACES
    : body.type === 'review' ? INDEX.REVIEWS
    : INDEX.PATTERNS

  const res = await chunkAndEmbed({
    id: `${body.type}:${body.id}`,
    text: body.text,
    meta: { type: body.type, ...(body.meta ?? {}) },
    indexName,
    chunk: { strategy: 'recursive', maxSize: 512, overlap: 50 },
    embed: { model: 'text-embedding-3-small', dimensions: 256 } // levnější index
  })

  await redis.setex(key, 60 * 60 * 24, '1') // denní TTL, stačí na deduplikaci toku
  return res
})
```

* `MDocument.chunk` a `embedMany` jsou použité přesně dle Mastra příkladů; nastavujeme menší rozměr 256 (OpenAI to podporuje). ([mastra.ai][3])

---

## 5) Query endpoint (retrieval + filtry)

```ts
// server/routes/search.post.ts
import { defineEventHandler, readBody } from 'h3'
import { retrieveCandidates } from '~/server/utils/retrieval'
import { INDEX } from '~/server/utils/vector'

export default defineEventHandler(async (e) => {
  const body = await readBody<{ q: string, filters?: Record<string, any> }>(e)

  const results = await retrieveCandidates({
    indexName: INDEX.PLACES,
    q: body.q,
    topK: 50,
    filter: body.filters
  })

  return { results }
})
```
